---
title: "Project Outline"
author: "Lea"
date: "1/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

In functional MRI multivoxel pattern analysis (MVPA), we are sometimes faced with classification problems where the training set contains unequal class sizes. For example, if we are trying to predict activity for remembered v. forgotten pictures, we may find that there are more remembered than forgotten trials. Unequal observations between the classes may bias the classifier to predict the more frequently observed class. In other words, if trained on more “remembered” trials, the classifier may be more likely to predict trials in the untrained set as “remembered.”

While subsampling is a common solution to unequal class sizes (REFS), it’s not always an ideal approach when working with fMRI data. The available training data in MVPA is often small and subsampling can exacerbate this problem. Additionally, classifiers are sometimes trained on repeated trials, which can also bias the classifiers if using a subsampling approach without properly balancing the training set. 


## Data

The data was taken from a previously conducted study that tested the effects of trial timing on fMRI pattern analysis (Zeithamova et al., 2017). While undergoing MRI, participants intentionally encoded pictures of animals and tools. There were five trial timing conditions, with two runs per condition (10 scanning runs total). For the purpose of this report, I will be including data from three of the trial timing conditions (Table 1). Following the MRI scan, memory for the items were tested in an old v. new recognition task. 

**Table 1.** Trial timing conditions
| Condition      | Trial Length | Item Repetitions* | Number of Trials* | Class Size |
|:---------------|:------------:|:-----------------:|:-----------------:|:----------:|
| Quick-Jittered | 4-8 s        | 4                 | 48                | 6          |
| Quick          | 6 s          | 3                 | 36                | 6          |
| Slow           | 12 s         | 2                 | 24                | 6          |
* These numbers are for a single run

For each run, a general linear model was fit to the functional MRI data to estimate the pattern of activation evoked by each item (e.g., rabbit, hammer, dog, screwdriver). This model produces a single beta estimate for each voxel (i.e., 3D pixel in the brain image). The pattern of beta estimates across voxels will serve as the data for the classification analyses. The voxels will be limited to specific brain regions (see below) and vectorized to serve as the "features" or input to the models. 

## Types of Classification Problems

We will train a machine learning algorithm to differentiate between distributed patterns of brain activity when:
+ a person is viewing animals v. tools (manipulate class sizes)
+ an item is remembered v. forgotten (more realistic example of unequal class sizes)


## Regions of Interest

For this report, I will compare performance of the models in 3 different regions of interest:
+ Lateral occipital cortex
+ Posterior fusiform gyrus
+ Inferior parietal cortex
**Will drop the different ROIs if it becomes too much, but I think it definitely addresses an important question about how distinct representations are in different brain regions**

## Overview of Model Procedures

I will run separate models for each of the three trial timing conditions. There are two functional runs for each trial timing condition. Each run will serve as a cross-validation fold. 

**What about feature selection? Can this be done with the model?**
+ will want to do feature selection, they took the top 100 voxels in original paper
+ SMLR does it automatically, need to find out if SVM will do it

**Do I need to scale the beta estimates?**
+ probably? I don't think they are standardized betas, but I can confirm

#### Performance metrics

Classification accuracy will be averaged across the two folds. 

**Is there another measure I could use? Confidence of the classifiers?**
**What about evaluating how well it can predict each class?? Cross-Matrix**

          | Guess Animal | Guess Tool
Is Animal |              |
Is Tool   |              |
  
#### Support Vector Regression

+ with linear kernel (not tuned)
+ which packages am I getting the model from (`caret`, `e1071`, etc.)? 
+ should I do any tuning? - the only parameter that can be tuned is the c parameter; tuning would require a leave-one-out approach which isn't great bc we only have class sizes of 6, also not often done in MVPA
+ For a first pass, I want to replicate the same model that gets run by default in the PyMVPA package, which is how the data were initially modeled

#### Other possible models

+ sparse multinomial logistic regression (SMLR) - the `msgl` package
+ elastic net or ridge regression
+ knn - probably not enough data

#### Class Size Ratios v. Sub-sampling

I test the robustness of each machine learning algorithm against unequal class sizes. I will manipulate the sizes of each class so they will be:
+ 1:1 (whole-sample)
+ 1:1 (sub-sample of 4)
+ 2:3 (4/6)
+ 1:2 (3/6)
+ 1:3 (2/6)

As I don't have any predictions about one category being more easily classified than the other, each category will be down sampled and results will be averaged across iterations for each ratio.

**it looks like with SVM I can also specify class weights when they're unequal**
+ I will look into this and might include this also as a comparison
trainsub <- list(trainsub)
# if the class sizes are not balanced
} else {
# determine size of the smaller class
subsize <- ratio[1] * (nrow(train)/ngroups)
# downsample the smaller class
trainsub <- list()
for (i in 1:times) {
# split training data
trainsplit <- train %>%
group_by_at(1) %>%
group_split()
# downsample group 1
sampGroup1 <- trainsplit[[1]] %>%
slice_sample(n = subsize)
# downsample group 2
sampGroup2 <- trainsplit[[2]] %>%
slice_sample(n = subsize)
trainsub[[i]] <- list(rbind(sampGroup1, trainsplit[[2]]),
rbind(trainsplit[[1]], sampGroup2))
}
trainsub <- flatten(trainsub)
}
return(trainsub)
}
# # troubleshoot
# trainsets <- subSample(folds[[1]][["train"]], c(.5,1), times = 2)
featSelect <- function(train, nfeatures) {
train_split <- train %>%
group_by_at(1) %>%
group_split()
fvals <- vector()
for (fs in 2:ncol(train)) {
tmp <- t.test(train_split[[1]][,fs],
train_split[[2]][,fs])
fvals[fs-1] <- tmp$statistic[[1]]^2
}
top_features <- sort(fvals, decreasing = TRUE, index.return = TRUE)
idx <- top_features$ix[1:nfeatures] + 1 # need to shift indices up 1 to match columns
}
# # troubleshoot
# topfeats <- map(trainsets, ~ featSelect(.x, nfeatures = 100))
featSelect <- function(train, nfeatures) {
train_split <- train %>%
group_by_at(1) %>%
group_split()
fvals <- vector()
for (fs in 2:ncol(train)) {
tmp <- t.test(train_split[[1]][,fs],
train_split[[2]][,fs])
fvals[fs-1] <- tmp$statistic[[1]]^2
}
top_features <- sort(fvals, decreasing = TRUE, index.return = TRUE)
idx <- top_features$ix[1:nfeatures] + 1 # need to shift indices up 1 to match columns
}
# # troubleshoot
# topfeats <- map(trainsets, ~ featSelect(.x, nfeatures = 100))
runSVM <- function(train, test) {
# # troubleshoot
# train <- folds[[1]][["train"]]
# test <- folds[[1]][["test"]]
# train model
x <- train[,-1]
y <- train[,1]
train_mdl <- e1071::svm(x, y,
scale = TRUE,
type = "C-classification",
kernel = "linear")
# predict test set
pred <- predict(train_mdl, test[,-1], decision.values = FALSE)
#decvals <- attr(pred, "decision.values")
y <- test[,1]
# evaluate performance
results <- caret::confusionMatrix(pred, y)
test_metrics <- tibble(acc = results$overall[["Accuracy"]],
hit_class1 = results$byClass[["Sensitivity"]],
hit_class2 = results$byClass[["Specificity"]],
fa_class1 = results$table[1,2]/sum(results$table[,2]),
fa_class2 = results$table[2,1]/sum(results$table[,2]))
return(test_metrics)
}
# # troubleshoot
# runSVM(train, test)
allConds <- expand.grid(ssids = ssids,
conds = conds,
rois = rois) %>%
mutate_if(is.factor, as.character)
tictoc::tic()
ratio11 <- pmap(allConds, ~{
compSVM(ssid = ..1,
cond = ..2,
roi = ..3,
ratio = c(1,1),
fold_by = "run",
dv = "category",
nsamples = 5,
nfeats = 100)})
compSVM <- function(ssid, cond, roi, ratio, fold_by, dv, nsamples, nfeats) {
# load data
df <- getData(ssid, cond, roi)
# create folds for cross-validation
folds <- cvfold(df, fold_by, dv)
cv_output <- list()
for (i in 1:length(folds)) {
# define training and testing sets
train <- folds[[i]][["train"]]
test <- folds[[i]][["test"]]
# subset training set based on ratios
trainsets <- subSample(train, ratio, times = nsamples)
# feature selection for each training set
topfeats <- map(trainsets, ~ featSelect(.x, nfeats))
# apply to each training set
trainsets_fs <- map2(trainsets, topfeats, ~{
.x[,c(1,.y)]
})
# create test sets with top features
testsets_fs <- map(topfeats, ~ select(test, c(1, all_of(.x))))
# run models
cv_output[[i]] <- map2_dfr(trainsets_fs, testsets_fs, ~ runSVM(.x, .y)) %>%
colMeans()
}
final_output <- bind_rows(cv_output) %>%
colMeans()
return(final_output)
}
# run the models with a 1:1 ratio
tictoc::tic()
ratio11 <- pmap(allConds, ~{
compSVM(ssid = ..1,
cond = ..2,
roi = ..3,
ratio = c(1,1),
fold_by = "run",
dv = "category",
nsamples = 5,
nfeats = 100)})
tictoc::toc()
View(ratio11)
ratios
tictoc::tic()
ratio55 <- pmap(allConds, ~{
compSVM(ssid = ..1,
cond = ..2,
roi = ..3,
ratio = c(.5,.5),
fold_by = "run",
dv = "category",
nsamples = 5,
nfeats = 100)})
tictoc::toc()
View(ratios)
ratios <- matrix(c(6,3,4,3,2,1,
6,3,6,6,6,6),
nrow = 6, ncol = 2)
ratios
# run the models with a 2:3 ratio
tictoc::tic()
tictoc::tic()
ratio23 <- pmap(allConds, ~{
compSVM(ssid = ..1,
cond = ..2,
roi = ..3,
ratio = c(2/3,1),
fold_by = "run",
dv = "category",
nsamples = 5,
nfeats = 100)})
tictoc::toc()
3553.266/60
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
#library(magrittr)
#install.packages("e1071")
#library(e1071)
#install.packages("caret")
# Specify subject ids
ssids <- seq(1,35)
ssids <- str_pad(ssids, width = 2, side = "left", pad = "0")
nsub <- length(ssids)
# Specify trial timing conditions
conds <- c("quick4n","quick6","slow12")
nconds <- length(conds)
# Specify regions of interest (ROIs)
rois <- c("lo","ofus")
# rois <- c("lo","pfus","ipar")
nrois <- length(rois)
# Specify subsampling ratios
ratios <- matrix(c(6,3,4,3,2,1,
6,3,6,6,6,6),
nrow = 6, ncol = 2)
nratios <- nrow(ratios)
View(ratio55)
View(allConds)
View(allConds)
ratios
tictoc::tic()
ratio23 <- pmap(allConds, ~{
compSVM(ssid = ..1,
cond = ..2,
roi = ..3,
ratio = c(.5,1),
fold_by = "run",
dv = "category",
nsamples = 5,
nfeats = 100)})
tictoc::toc()
5074.025/60
rios <- c("lo")
rm(rios)
rois <- c("lo")
ratios
tictoc::tic()
ratio23 <- pmap(allConds, ~{
compSVM(ssid = ..1,
cond = ..2,
roi = ..3,
ratio = c(1/3,1),
fold_by = "run",
dv = "category",
nsamples = 5,
nfeats = 100)})
tictoc::toc()
4966.949/60
saveRDS(ratio23, "models/ratio23")
saveRDS(ratio5, "models/ratio55")
saveRDS(ratio55, "models/ratio55")
saveRDS(ratio11, "models/ratio11")
View(allConds)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
#library(magrittr)
#install.packages("e1071")
#library(e1071)
#install.packages("caret")
# Specify subject ids
ssids <- seq(1,35)
ssids <- str_pad(ssids, width = 2, side = "left", pad = "0")
nsub <- length(ssids)
# Specify trial timing conditions
conds <- c("quick4n","quick6","slow12")
nconds <- length(conds)
# Specify regions of interest (ROIs)
rois <- c("lo")
# rois <- c("lo","pfus","ipar")
nrois <- length(rois)
# Specify subsampling ratios
ratios <- matrix(c(6,3,4,3,2,1,
6,3,6,6,6,6),
nrow = 6, ncol = 2)
nratios <- nrow(ratios)
getData <- function(ssid, cond, roi) {
# load behavioral data
fname <- sprintf("beh/sub-%s_task-study_items.tsv", ssid)
beh <- rio::import(fname) %>%
mutate(design = factor(design, labels = c("med8","quick4n","quick6",
"slow10","slow12")),
category = factor(category, labels = c("animal","tool")))
# load betas for each voxel
## exclude first 3 rows, just coordinates of voxel
fname <- sprintf("betas/item_betas_sub_%s_%s_b_%s.txt", ssid, cond, roi)
betas <- rio::import(fname) %>%
slice(-1:-3)
# Combine data for classification
df <- cbind(beh, betas) %>%
filter(design == cond)
return(df)
}
# # troubleshoot
# df <- getData(ssid = "01",
#               cond = "quick4n",
#               roi = "lo")
cvfold <- function(df, fold_by, dv) {
# # make these executable in the code below
#fold_by <- eval(substitute(fold_by), df, parent.frame())
# number of folds should equal number of levels
num_folds <- length(unique(df[,fold_by]))
# create train/test set for each fold
folds <- list()
for (i in c(1:num_folds)) {
train <- df %>%
filter_at(vars(contains(fold_by)), ~ .x != i) %>%
select(dv, contains("V"))
test <- df %>%
filter_at(vars(contains(fold_by)), ~ .x == i) %>%
select(dv, contains("V"))
folds[[i]] <- list(train = train,
test = test)
}
return(folds)
}
# # troubleshoot
# folds <- cvfold(df, fold_by = "run", dv = "category")
subSample <- function(train, ratio, times, ngroups = 2) {
# get number of observations per class
class_size <- nrow(train)/ngroups
# if the class sizes are balanced
if (ratio[1] == ratio[2]) {
# determine size of each class
class_size <- ratio[1] * (nrow(train)/ngroups)
# downsample each class
trainsub <- train %>%
group_by_at(1) %>%
slice_sample(n = class_size) %>%
ungroup()
trainsub <- list(trainsub)
# if the class sizes are not balanced
} else {
# determine size of the smaller class
subsize <- ratio[1] * (nrow(train)/ngroups)
# downsample the smaller class
trainsub <- list()
for (i in 1:times) {
# split training data
trainsplit <- train %>%
group_by_at(1) %>%
group_split()
# downsample group 1
sampGroup1 <- trainsplit[[1]] %>%
slice_sample(n = subsize)
# downsample group 2
sampGroup2 <- trainsplit[[2]] %>%
slice_sample(n = subsize)
trainsub[[i]] <- list(rbind(sampGroup1, trainsplit[[2]]),
rbind(trainsplit[[1]], sampGroup2))
}
trainsub <- flatten(trainsub)
}
return(trainsub)
}
# # troubleshoot
# trainsets <- subSample(folds[[1]][["train"]], c(.5,1), times = 2)
featSelect <- function(train, nfeatures) {
train_split <- train %>%
group_by_at(1) %>%
group_split()
fvals <- vector()
for (fs in 2:ncol(train)) {
tmp <- t.test(train_split[[1]][,fs],
train_split[[2]][,fs])
fvals[fs-1] <- tmp$statistic[[1]]^2
}
top_features <- sort(fvals, decreasing = TRUE, index.return = TRUE)
idx <- top_features$ix[1:nfeatures] + 1 # need to shift indices up 1 to match columns
}
# # troubleshoot
# topfeats <- map(trainsets, ~ featSelect(.x, nfeatures = 100))
runSVM <- function(train, test) {
# # troubleshoot
# train <- folds[[1]][["train"]]
# test <- folds[[1]][["test"]]
# train model
x <- train[,-1]
y <- train[,1]
train_mdl <- e1071::svm(x, y,
scale = TRUE,
type = "C-classification",
kernel = "linear")
# predict test set
pred <- predict(train_mdl, test[,-1], decision.values = FALSE)
#decvals <- attr(pred, "decision.values")
y <- test[,1]
# evaluate performance
results <- caret::confusionMatrix(pred, y)
test_metrics <- tibble(acc = results$overall[["Accuracy"]],
hit_class1 = results$byClass[["Sensitivity"]],
hit_class2 = results$byClass[["Specificity"]],
fa_class1 = results$table[1,2]/sum(results$table[,2]),
fa_class2 = results$table[2,1]/sum(results$table[,2]))
return(test_metrics)
}
# # troubleshoot
# runSVM(train, test)
compSVM <- function(ssid, cond, roi, ratio, fold_by, dv, nsamples, nfeats) {
# load data
df <- getData(ssid, cond, roi)
# create folds for cross-validation
folds <- cvfold(df, fold_by, dv)
cv_output <- list()
for (i in 1:length(folds)) {
# define training and testing sets
train <- folds[[i]][["train"]]
test <- folds[[i]][["test"]]
# subset training set based on ratios
trainsets <- subSample(train, ratio, times = nsamples)
# feature selection for each training set
topfeats <- map(trainsets, ~ featSelect(.x, nfeats))
# apply to each training set
trainsets_fs <- map2(trainsets, topfeats, ~{
.x[,c(1,.y)]
})
# create test sets with top features
testsets_fs <- map(topfeats, ~ select(test, c(1, all_of(.x))))
# run models
cv_output[[i]] <- map2_dfr(trainsets_fs, testsets_fs, ~ runSVM(.x, .y)) %>%
colMeans()
}
final_output <- bind_rows(cv_output) %>%
colMeans()
return(final_output)
}
df <- getData(ssid = "01",
cond = "quick4n",
roi = "lo")
folds <- cvfold(df,
fold_by = "run",
dv = "category")
i <- 1
# specify all model conditions
allConds <- expand.grid(ssids = ssids,
conds = conds,
rois = rois) %>%
mutate_if(is.factor, as.character)
View(allConds)
View(folds)
View(allConds)
View(df)
###########
## Setup ##
###########
library(tidyverse)
source("mvpaFunctions.R")
# Specify subject ids
ssids <- seq(1,35)
ssids <- str_pad(ssids, width = 2, side = "left", pad = "0")
nsub <- length(ssids)
rm(nsub)
# Specify subject ids
ssids <- str_pad(seq(1,35), width = 2, side = "left", pad = "0")
# Specify ratios for different class sizes
ratios <- matrix(c(1,.5,2,1,2,
1,.5,3,2,6),
nrow = 6, ncol = 2)
# Specify ratios for different class sizes
ratios <- matrix(c(1,.5,2,1,2,
1,.5,3,2,6),
nrow = 5, ncol = 2)
ratios
# Specify ratios for different class sizes
ratios <- c("c(1,1)",
"c(.5,.5)",
"c(2/3,1)",
"c(1/2,1)",
"c(1/3,1)")
ratios
parse(test = ratios[1])
parse(text = ratios[1])
eval(parse(text = ratios[1]))
# Specify subject ids
ssids <- str_pad(seq(1,35), width = 2,
side = "left", pad = "0")
# Specify trial timing conditions
timing <- c("slow12") # c("quick4n","quick6","slow12")
# Specify regions of interest (ROIs)
rois <- c("lo") # c("lo","pfus","ipar")
# Specify ratios for different class sizes
ratios <- c("c(1,1)",
"c(.5,.5)",
"c(2/3,1)",
"c(1/2,1)",
"c(1/3,1)")
rm(timing)
## Create Data Frame for Conditions ##
conds <- expand.grid(ssid = ssids,
timing = timings,
roi = rois,
ratio = ratios)
# Specify trial timing conditions
timings <- c("slow12") # c("quick4n","quick6","slow12")
## Create Data Frame for Conditions ##
conds <- expand.grid(ssid = ssids,
timing = timings,
roi = rois,
ratio = ratios)
View(conds)
readRDS("models/ratio11")
ratio11 <- readRDS("models/ratio11")
View(ratio11)
rm(ratio11)
## Add Data ##
conds <- conds %>%
rowwise() %>%
mutate(df = list(getData(ssid, timing, roi)))
## Add Data ##
conds <- conds %>%
rowwise() %>%
mutate(df = list(getData(ssid, timing, roi)))
## Create Data Frame for Conditions ##
conds <- expand.grid(ssid = ssids,
timing = timings,
roi = rois,
ratio = ratios) %>%
# not sure why creating factors
mutate_if(is.factor, as.character)
## Add Data ##
conds <- conds %>%
rowwise() %>%
mutate(df = list(getData(ssid, timing, roi)))
View(conds)
conds$df[[1]]
View(conds$df[[1]])
View(conds$df[[2]])
View(conds$df[[3]])
## Create Folds ##
conds <- conds %>%
rowwise() %>%
mutate(folds = list(cvfold(df,
fold_by = "run",
dv = "category")))
# install.packages("e1071")
source("mvpaFunctions.R")
